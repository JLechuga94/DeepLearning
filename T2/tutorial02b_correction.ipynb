{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic generation of image-captions\n",
    "\n",
    "![alt text](model.png)\n",
    "\n",
    "- Today's tutorial aims at showing you the capabilities that neural networks have when dealing with images and language. \n",
    "- We will see how to use some pre-trained neural networks to program our image-caption generator: we will create a simple system which given any image as input, will output a brief description of the image. \n",
    "- This system will consist of two neural networks, the first one will be responsible for dealing with the visual part of the problem (called the encoder), while the second one with the linguistic one (called the decoder). \n",
    "- A part of your work is also to deal with any missing library, using `conda install` or `pip install` whatever is missing\n",
    "\n",
    "We will **not train these neural networks ourselves**, therefore you will need to have downloaded the following files: \n",
    "\n",
    "- `encoder.pkl` : a pickle file containing the weights of the encoder\n",
    "- `encoder.pkl` : a pickle file containing the weights of the decoder\n",
    "- `vocabulary.pkl` : a pickle file containing the vocabulary used\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:45.473772Z",
     "start_time": "2021-02-04T15:44:44.929820Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:45.477870Z",
     "start_time": "2021-02-04T15:44:45.475561Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load an image we want to generate a caption for\n",
    "\n",
    "- Write a custom function which given a path to an image loads it with the PIL library (see imports above and https://he-arc.github.io/livre-python/pillow/index.html)\n",
    "- The image should be resized to a 224x224 pixels image\n",
    "- Explore the type of the object which gets loaded, do you think it is possible to use such an object as input to a neural network? Why/Why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:45.898570Z",
     "start_time": "2021-02-04T15:44:45.895473Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = image.resize([224, 224], Image.LANCZOS)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:46.070530Z",
     "start_time": "2021-02-04T15:44:46.042738Z"
    }
   },
   "outputs": [],
   "source": [
    "image_path = \"giraffe.png\"\n",
    "image = load_image(image_path)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:46.206177Z",
     "start_time": "2021-02-04T15:44:46.202487Z"
    }
   },
   "outputs": [],
   "source": [
    "type(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the loaded image to a tensor\n",
    "\n",
    "An `Image` object cannot be used as input for a neural network. We need in fact to convert it to a tensor and normalize the values of its pixels\n",
    "\n",
    "- Write a custom function that gets a PIL Image as input, normalizes its pixels and converts the image to a tensor which can be used as input for a neural network. For this, use the `torchvision` library, in particular its `transforms` module, and the `Compose`, `ToTensor` and `Normalize` classes, among others. \n",
    "- **Warning**: You should use `ImageNet`'s means and standard deviations to normalize each channel, look what this means and code it!\n",
    "- The size of the output tensor should be `(1, 3, 244, 244)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:46.514614Z",
     "start_time": "2021-02-04T15:44:46.511183Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "def preprocess_image(image):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:46.675270Z",
     "start_time": "2021-02-04T15:44:46.670641Z"
    }
   },
   "outputs": [],
   "source": [
    "image_tensor = preprocess_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:46.851404Z",
     "start_time": "2021-02-04T15:44:46.847862Z"
    }
   },
   "outputs": [],
   "source": [
    "image_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:47.019711Z",
     "start_time": "2021-02-04T15:44:47.015863Z"
    }
   },
   "outputs": [],
   "source": [
    "image_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:47.191275Z",
     "start_time": "2021-02-04T15:44:47.188390Z"
    }
   },
   "outputs": [],
   "source": [
    "print(type(image_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the vocabulary of words which will be used for creating a caption\n",
    "\n",
    "The neural network will require a pool of words to choose from to come up with an appropriate caption for an image. \n",
    "\n",
    "- Write a function that loads the pickle file and returns the \"vocabulary\" object\n",
    "- Look at the attributes contained in this \"vocabulary\". What are those ? What do they do ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:47.548726Z",
     "start_time": "2021-02-04T15:44:47.545745Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "def load_vocabulary(vocab_path):\n",
    "    with open(vocab_path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "        \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:47.722762Z",
     "start_time": "2021-02-04T15:44:47.716061Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_path = \"vocabulary.pkl\"\n",
    "vocab = load_vocabulary(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:47.943943Z",
     "start_time": "2021-02-04T15:44:47.888075Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:48.087248Z",
     "start_time": "2021-02-04T15:44:48.083906Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(vocab[\"idx2word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:48.252149Z",
     "start_time": "2021-02-04T15:44:48.246334Z"
    }
   },
   "outputs": [],
   "source": [
    "{k: v for i, (k, v) in enumerate(vocab[\"idx2word\"].items()) if i <= 15}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T13:03:40.052138Z",
     "start_time": "2021-02-04T13:03:40.048467Z"
    }
   },
   "source": [
    "We see that\n",
    "\n",
    "- `vocab[\"idx2word\"]` contains a mapping idx: word\n",
    "- `vocab[\"word2idx\"]` contains a mapping word: idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the neural network architecture \n",
    "\n",
    "Here we create the computational graph of the neural architecture which will get an image as input and outputs an appropriate caption.\n",
    "\n",
    "- For today, we provide the code for this. You won't understand most of it, but a good exercice to learn is not \"reverse engineer\" it: try parts of the code, look at the documentation, to try understand what this code does\n",
    "- The neural network consists of two different parts: an **encoder**, which uses a ResNet-152 architecture (it's a big convolutional network) and a **decoder**, which uses a simple LSTM (a kind of recurrent neural network).\n",
    "\n",
    "Note that the code below only create the computational graphs of the networks architectures, but they are not trained at all, and are kind of empty shells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:50.083649Z",
     "start_time": "2021-02-04T15:44:48.793568Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"Loads the pretrained ResNet-152 but replaces the fully-connected layer.\n",
    "        \"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        # Delete the last fully-connected layer\n",
    "        modules = list(resnet.children())[:-1]     \n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.bn(self.linear(features))\n",
    "        return features\n",
    "    \n",
    "def build_encoder(embedding_size):\n",
    "    # Use eval mode (because of batch-normalization)\n",
    "    encoder = EncoderCNN(embedding_size).eval()  \n",
    "    encoder = encoder.to(device)\n",
    "    return encoder \n",
    "\n",
    "embedding_size = 256\n",
    "encoder = build_encoder(embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:50.097820Z",
     "start_time": "2021-02-04T15:44:50.085856Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:50.168195Z",
     "start_time": "2021-02-04T15:44:50.100649Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_seg_length = max_seq_length\n",
    "        \n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, features, states=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        for i in range(self.max_seg_length):\n",
    "            hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
    "            _, predicted = outputs.max(1)                        # predicted: (batch_size)\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)\n",
    "            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
    "        return sampled_ids\n",
    "\n",
    "\n",
    "def build_decoder(embedding_size, hidden_size, vocab, num_layers):\n",
    "    decoder = DecoderRNN(embedding_size, hidden_size, len(vocab[\"idx2word\"]), num_layers)\n",
    "    decoder = decoder.to(device)\n",
    "    return decoder\n",
    "\n",
    "\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "decoder = build_decoder(embedding_size, hidden_size, vocab, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:50.173177Z",
     "start_time": "2021-02-04T15:44:50.170044Z"
    }
   },
   "outputs": [],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the networks weights within the computational graph\n",
    "\n",
    "The neural networks created above are empty shells: they are not trained, meaning that the captions which we would obtain as output would be garbage. \n",
    "We need to load the appropriate weights within the computation graph. \n",
    "\n",
    "- Write a function which given a computational graph loads and the already trained weights that you have already downloaded. Pickle files (`encoder-*.pkl` and `decoder-*.pkl` files) can be loaded with `torch.load`. The files contain the `state_dict` or the networks, that can be passed through the `load_state_dict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:50.455287Z",
     "start_time": "2021-02-04T15:44:50.174687Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "def load_weights(model, weights_path):\n",
    "    model.load_state_dict(torch.load(weights_path))    \n",
    "    return model\n",
    "\n",
    "encoder = load_weights(encoder, 'encoder.pkl')\n",
    "decoder = load_weights(decoder, 'decoder.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract meaningful information from the encoder\n",
    "\n",
    "The encoder aims at converting the input image into a feature vector that represents the original image in a \"meaningful\" way. This corresponds to a *forward pass* in the encoder network. \n",
    "\n",
    "- Write a function that gets as input an image, the encoder-network and returns a feature vector.\n",
    "\n",
    "- What is the difference between the extracted features and the original input of the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:50.460589Z",
     "start_time": "2021-02-04T15:44:50.457371Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "def extract_features(image_tensor, encoder):\n",
    "    features = encoder(image_tensor)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:50.906527Z",
     "start_time": "2021-02-04T15:44:50.463211Z"
    }
   },
   "outputs": [],
   "source": [
    "features = extract_features(image_tensor, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:50.911383Z",
     "start_time": "2021-02-04T15:44:50.908442Z"
    }
   },
   "outputs": [],
   "source": [
    "features.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:50.918539Z",
     "start_time": "2021-02-04T15:44:50.912958Z"
    }
   },
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match the extracted image features with a potential caption\n",
    "\n",
    "Now, we want to use the features extracted from the image by the encoder as input for the decoder. The goal is to get from the decoder a set of words which corresponds to a relevant caption of the image. \n",
    "\n",
    "- Write a function which uses the decoder and the extracted image features to returns what will be later the words of the caption\n",
    "\n",
    "**Hint**: the decoder will call the `.sample()` method defined above. Also, the output should no longer be a `pytorch` tensor, but rather a `numpy` array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:51.046797Z",
     "start_time": "2021-02-04T15:44:51.000300Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_word_idxs(decoder, features):\n",
    "    word_idxs = decoder.sample(features)\n",
    "    word_idxs = word_idxs[0].cpu().numpy()    \n",
    "    return word_idxs\n",
    "\n",
    "word_idxs = sample_word_idxs(decoder, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:44:51.195760Z",
     "start_time": "2021-02-04T15:44:51.192158Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the output of the decoder to meaningful words\n",
    "\n",
    "The decoder network does not output directly words but a list of integerers instead.\n",
    "These integers are actually indexes for words from the vocabulary you loaded before.\n",
    "\n",
    "- Write a function that converts the output of the decoder into meaningful words using the vocabulary\n",
    "- If you meet the word `<end>` then you should stop the sentence\n",
    "- Try to do this in the most Pythonic way possible (using the modules from the standard library, such as `itertools`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:45:00.435128Z",
     "start_time": "2021-02-04T15:45:00.430149Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import takewhile\n",
    "\n",
    "def create_caption(vocab, word_idxs):\n",
    "    return \" \".join(list(map(vocab[\"idx2word\"].get, \n",
    "                             takewhile(lambda idx: idx != 2, word_idxs))))\n",
    "\n",
    "create_caption(vocab, word_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all this together\n",
    "\n",
    "Now, you are able to load an image and generated a captioning of it.\n",
    "\n",
    "- Write a function that takes as input an image and returns a readable caption about it. This function should take as input both the encoder and decoder (with loaded weights) and the vocabulary. Of course, you can use in this function some of the functions defined above. Try to avoid the use of objects from the global namespace (pass all useful objects as arguments). It's a good practice to follow, in other to avoid bugs.\n",
    "- Try your function on several images ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:45:01.232756Z",
     "start_time": "2021-02-04T15:45:01.229492Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_caption(image, encoder, decoder, vocab):\n",
    "    image_tensor = preprocess_image(image)\n",
    "    features = extract_features(image_tensor, encoder)\n",
    "    word_idxs = sample_word_idxs(decoder, features)\n",
    "    caption = create_caption(vocab, word_idxs)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:45:01.680800Z",
     "start_time": "2021-02-04T15:45:01.660375Z"
    }
   },
   "outputs": [],
   "source": [
    "image = load_image('giraffe.png')\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:45:02.289597Z",
     "start_time": "2021-02-04T15:45:01.839814Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_caption(image, encoder, decoder, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just for fun: try on any image (using your webcam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:45:02.579230Z",
     "start_time": "2021-02-04T15:45:02.527790Z"
    }
   },
   "outputs": [],
   "source": [
    "# Might require pip install opencv-python\n",
    "import cv2\n",
    "\n",
    "def camera_grab(camera_id=0):\n",
    "    \"\"\"This function captures an image through the webcam of your laptop (if accessible)\n",
    "    \"\"\"\n",
    "    camera = cv2.VideoCapture(camera_id)\n",
    "    try:\n",
    "        for i in range(10):\n",
    "            snapshot_ok, image = camera.read()\n",
    "        if not snapshot_ok:\n",
    "            raise RuntimeError(\"Could not access camera\")\n",
    "    finally:\n",
    "        camera.release()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:45:05.514443Z",
     "start_time": "2021-02-04T15:45:02.957095Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image = camera_grab()\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T15:45:06.051517Z",
     "start_time": "2021-02-04T15:45:05.517025Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write the image on disk\n",
    "plt.imsave(\"webcam.jpg\", cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Read again the image\n",
    "image = load_image(\"webcam.jpg\")\n",
    "\n",
    "# And generate the caption\n",
    "generate_caption(image, encoder, decoder, vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "330.7201232910156px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
