{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym # openAi gym\n",
    "from gym import envs\n",
    "import numpy as np \n",
    "from numpy import linalg as LA\n",
    "import datetime\n",
    "import keras \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "\n",
    "import os\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install keras-rl\n",
    "!conda install -c conda-forge gym-box2d\n",
    "!sudo apt install cmake libz-dev # atari-py depends on cmake if its not installed atari-py installation will fail\n",
    "!pip install atari-py\n",
    "!pip install nes-py\n",
    "!pip install gym-super-mario-bros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras-rl implements some state-of-the art deep reinforcement learning algorithms in Python and seamlessly integrates with the deep learning library Keras.\n",
    "\n",
    "Furthermore, keras-rl works with OpenAI Gym out of the box. This means that evaluating and playing around with different algorithms is easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Reinforcement Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](./images/rl_loop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model based Value Iteration Algorithm \n",
    "Model free Q Learning\n",
    "\n",
    "https://arxiv.org/pdf/1312.5602.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word Markov refers to Markovian property which means that the state is independent of any previous states history, not on the sequence of events that preceded it. The current state encapsulates all that is needed to decide the future actions, no memory needed.\n",
    "\n",
    "In terms of Reinforcement Learning the Environment is modeled as a markov model and the agent needs to take actions in this environment to maximize the amount of reward. Since the agent sees only the outside of the environment (the effects of it actions) it is often referred to as the hidden markov model which needs to be learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy (π): The strategy that the agent employs to determine next action 'a' in state 's'. Note that it does not state if it is a good or bad policy, it is a policy. The policy is normally noted with the greek letter π. Optimal policy (π*), policy which maximizes the expected reward. Among all the policies taken, the optimal policy is the one that optimizes to maximize the amount of reward received or expected to receive over a lifetime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](./images/bellman_equation.svg)\n",
    "\n",
    "- R(s,a) = Reward of action a in state s\n",
    "- P(s'|s,a)= Probability of going to state s' given action a in state s. The Taxi game actions are deterministic (no such a thing as if I want to go north there is an 80% chance to go north and 10% chance to go west and 10% chance to go east). so the probability that selected action will lead to expected state is 100%. So ignore it for this game, it is always 1.\n",
    "- γ = Discount factor gamma, how much discount is applicable for the future rewards. It must be between 0 and 1. The higher gamma the higher the focus on long term rewards\n",
    "\n",
    "The value iteration algorithm makes use of the equation in the form of:\n",
    "\n",
    "- Value V(s): The expected long-term return with discount, as opposed to the short-term reward R. Vπ(s) is defined as the expected long-term return of the current state sunder policy π.\n",
    "The Q learning algorithm makes use of the equation in the form of:\n",
    "\n",
    "- Q-matrix or action-value Q(s,a): Q-matrix is similar to Value, except that it takes an extra parameter, the action a. Qπ(s, a) refers to the long-term return of the current state s, taking action a under policy π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Iteration Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value iteration is centred around the game states. The core of the idea is to calculate the value (expected long-term maximum result) of each state. The algorithm loops over all states (s) and possible actions (a) to explore rewards of a given action and calculates the maximum possible action/reward and stores it in V[s]. The algorithm iterates/repeats until V[s] is not (significantly) improving anymore. \n",
    "\n",
    "The Optimal policy (π*) is then to take every time the action to go state with the highest value. This value iteration algorithm is an example of what is referred to as dynamic programming (DP) in literature. There are other DP techniques to solve this like policy iteration, etc but it can also be solved by a recursive program (a function that calls itself, look at the Bellman equation, it is a recursive definition)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centered around the agent and starts exploring based on trial-and-error to update its knowledge about the model and hence path to the best reward. The core of the idea is the Q-matrix \n",
    "- Q(s, a). It contains the maximum discounted future reward when we perform action a in state s. Or in other words Q(s, a) gives estimates the best course of action a in state s. Q-learning learns by trail and error and updates its policy (Q-matrix) based on reward. to state it simple: the best it can do given a state it is in.\n",
    "\n",
    "After every step we update Q(s,a) using the reward, and the max Q value for new state resulting from the action. \n",
    "\n",
    "This update is done using the action value formula, based upon the Bellman equation and allows state-action pairs to be updated in a recursive fashion (based on future values).\n",
    "\n",
    "The Bellman equation is extended with a learning rate alpha:\n",
    "\n",
    "![image info](./images/q_learning_bellman.png)\n",
    "\n",
    "If alpha is set to 1, we return to the original Bellman equation.\n",
    "\n",
    "The Q-matrix is initialized with zero's. So initially it starts moving randomly until it hits a state/action with rewards or state/actions with a penalty. For understanding, let's simplify the problem that it needs to go to a certain drop-off position to get a reward. So random moves get no rewards but by luck (brute force enough tries) the state/action is found where a reward is given. So next game the immediate actions preceding this state/action will direct toward it by use of the Q-Matrix. The next iteration the actions before that, etc, etc. In other words, it solves \"the puzzle\" backwards from end-result (drop-off passenger) towards steps to be taken to get there in a iterative fashion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Exploration vs Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Exploitation: Make the best decision given current information available (Go to the restaurant you know you like)\n",
    "- Exploration: Gather more information (go to a new restaurant to find out if you like it)\n",
    "\n",
    "Some approaches for this tradeoff:\n",
    "\n",
    "- Epsilon Greedy: We exploit the current situation with probability 1 — epsilon and explore a new option with probability epsilon, the rates of exploration and exploitation are fixed\n",
    "- Epsilon-Decreasing: We exploit the current situation with probability 1 — epsilon and explore a new option with probability epsilon, with epsilon decreasing over time.\n",
    "- Thompson sampling: the rates of exploration and exploitation are dynamically updated with respect to the entire probability distribution of each arm\n",
    "- Epsilon-Decreasing with Softmax: We exploit the current situation with probability 1 — epsilon and explore a new option with probability epsilon, with epsilon decreasing over time. In the case of exploring a new option, we don’t just pick an option at random, but instead we estimate the outcome of each option, and then pick based on that, this is the softmax part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you got a full Atari game screen of pixels as an observation and it becomes quickly visible the Q-matrix solution will not cope. Also the Q-learning agent does not have the ability to estimate value for unseen states, it has no clue which action to take and goes back to random action as best.\n",
    "\n",
    "To deal with these problems, Deep Q-Network (DQN) removes the two-dimensional Q-matrix by introducing a Neural Network. So it leverages a Neural Network to estimate the Q-value function. The input for the network is the current game state, while the output is the corresponding Q-value for each of the actions.\n",
    "\n",
    "In 2014 Google DeepMind published a paper titled \"Playing Atari with Deep Reinforcement Learning\" that can play Atari 2600 games at expert human levels. This was the first breakthruogh in applying deep neural networks for reinforcement learning.\n",
    "\n",
    "![image info](./images/atari_dqn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym\n",
    "\n",
    "Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Pinball.\n",
    "\n",
    "![image info](./images/rl_gym_loop.png)\n",
    "\n",
    "Each timestep, the agent chooses an action, and the environment returns an observation and a reward.\n",
    "\n",
    "observation, reward, done, info = env.step(action)\n",
    "\n",
    "- observation (object): an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game line Taxi.\n",
    "- reward (float): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n",
    "- done (boolean): whether it’s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\n",
    "- info (dict): ignore, diagnostic information useful for debugging. Official evaluations of your agent are not allowed to use this for learning.\n",
    "\n",
    "There are a lot of different video game environments for testing Reinforcement Learning algorithms in different scenarios. Currently the number of different environments available in Python Gym is 859\n",
    "\n",
    "Some of the environments available are:\n",
    "\n",
    "Taxi, Mountaincar, InvertedPendulum, MarioBros, Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "859\n"
     ]
    }
   ],
   "source": [
    "print(len(envs.registry.all()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action (a): the input the agent provides to the environment. So what are the action commands the agents can give to the enironment? The value of env.action_space gives us this information.\n",
    "\n",
    "What is the meaning of the actions? For the deep learning algorithm it should not matter, it should sort it out independent of the meaning of the action. But for humans it is handy to have the description, so we can understand the actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State (s): This represents the board state of the game and in gym returned it is returned as observation. State: a numeric representation of what the agent is observing at a particular moment of time in the environment.\n",
    "In case of Taxi the observation is an integer, 500 different states are possible that translate to a nice graphic visual format with the render function. Note that this is specific for the Taxi game, in case of e.g. an Atari game the observation is the game screen with many coloured pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxi v3\n",
    "\n",
    "This task was introduced in [Dietterich2000] to illustrate some issues in hierarchical reinforcement learning. There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another. You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\n",
    "\n",
    "There are four designated locations in the grid world indicated by R(ed), B(lue), G(reen), and Y(ellow). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drive to the passenger's location, pick up the passenger, drive to the passenger's destination (another one of the four specified locations), and then drop off the passenger. Once the passenger is dropped off, the episode ends. The taxi cannot pass thru a wall.\n",
    "\n",
    "Rewards: There is a reward of -1 for each action and an additional reward of +20 for delievering the passenger. There is a reward of -10 for executing actions \"pickup\" and \"dropoff\" illegally.\n",
    "\n",
    "- blue: passenger\n",
    "- magenta: destination\n",
    "- yellow: empty taxi\n",
    "- green: full taxi\n",
    "- other letters: locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| :\u001b[43m \u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 6 operations the \"agent\" can perform:\n",
    "\n",
    "- 0: move south\n",
    "- 1: move north\n",
    "- 2: move east \n",
    "- 3: move west \n",
    "- 4: pickup passenger\n",
    "- 5: dropoff passenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible actions for agent in game Taxi: 6\n"
     ]
    }
   ],
   "source": [
    "# action space has 6 possible actions, the meaning of the actions is nice to know for us humans but the neural network will figure it out\n",
    "print(\"Possible actions for agent in game Taxi: %a\" % (env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(500)\n",
      "\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: |\u001b[43m \u001b[0m: :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print()\n",
    "env.env.s = 42 # random state\n",
    "env.render()\n",
    "env.env.s = 222 # random state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model based approach - Value Iteration Algorithm\n",
    "\n",
    "The Taxi game is an example of an Markov decision process . We can describe the game in states, possible actions in a statethat leading to a next state with a certain probability with a reward. In our case we will be solving the game via a value iteration algorithm using the Bellman equation described earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "Reward: -21\n"
     ]
    }
   ],
   "source": [
    "# Let's do some random steps in the game so we can see how the game looks like\n",
    "env = gym.make('Taxi-v3')\n",
    "env.reset()\n",
    "\n",
    "reward = 0\n",
    "obs = env.reset()\n",
    "print(\"Initial state\")\n",
    "env.render()\n",
    "for _ in range(3):\n",
    "    action = env.action_space.sample() #take step using random action from possible actions (actio_space)\n",
    "    obs, rew, done, info = env.step(action) \n",
    "    reward = reward + rew\n",
    "    env.render()\n",
    "#Final reward of these random actions\n",
    "\n",
    "print(\"Reward: %r\" % reward)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now apply our value iteration algorithm and analyze the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 41 iterations\n"
     ]
    }
   ],
   "source": [
    "# Value iteration algorithm\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n\n",
    "V = np.zeros([n_states]) # The Value for each state\n",
    "Pi = np.zeros([n_states], dtype=int)  # Our policy with we keep updating to get the optimal policy\n",
    "gamma = 0.9 # discount factor\n",
    "significant_improvement = 0.01\n",
    "\n",
    "def best_action_value(s):\n",
    "    # finds the highest value action (max_a) in state s\n",
    "    best_a = None\n",
    "    best_value = float('-inf')\n",
    "\n",
    "    # loop through all possible actions to find the best current action\n",
    "    for a in range (0, n_actions):\n",
    "        env.env.s = s\n",
    "        s_new, rew, done, info = env.step(a) #take the action\n",
    "        v = rew + gamma * V[s_new]\n",
    "        if v > best_value:\n",
    "            best_value = v\n",
    "            best_a = a\n",
    "    return best_a\n",
    "\n",
    "iteration = 0\n",
    "while True:\n",
    "    # biggest_change is referred to by the mathematical symbol delta in equations\n",
    "    biggest_change = 0\n",
    "    for s in range (0, n_states):\n",
    "        old_v = V[s]\n",
    "        action = best_action_value(s) #choosing an action with the highest future reward\n",
    "        env.env.s = s # goto the state\n",
    "        s_new, rew, done, info = env.step(action) #take the action\n",
    "        V[s] = rew + gamma * V[s_new] #Update Value for the state using Bellman equation\n",
    "        Pi[s] = action\n",
    "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "    iteration += 1\n",
    "    if biggest_change < significant_improvement:\n",
    "        print('Finished in {} iterations'.format(iteration))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "|\u001b[43m \u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[42mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "|\u001b[42m_\u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : |\u001b[42m_\u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: |\u001b[42m_\u001b[0m: :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | :\u001b[42m_\u001b[0m:\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Reward: 10\n"
     ]
    }
   ],
   "source": [
    "# Let's see how the algorithm solves the taxi game\n",
    "reward = 0\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "\n",
    "while done != True: \n",
    "    action = Pi[obs]\n",
    "    obs, rew, done, info = env.step(action) #take step using selected action\n",
    "    reward = reward + rew\n",
    "    env.render()\n",
    "#Print the reward of these actions\n",
    "print(\"Reward: %r\" % reward)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model free approach - Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50 Total Reward: -200\n",
      "Episode 100 Total Reward: -30\n",
      "Episode 150 Total Reward: 10\n",
      "Episode 200 Total Reward: -49\n",
      "Episode 250 Total Reward: -3\n",
      "Episode 300 Total Reward: 12\n",
      "Episode 350 Total Reward: 7\n",
      "Episode 400 Total Reward: 11\n",
      "Episode 450 Total Reward: 6\n",
      "Episode 500 Total Reward: 2\n",
      "Episode 550 Total Reward: 6\n",
      "Episode 600 Total Reward: 11\n",
      "Episode 650 Total Reward: 8\n",
      "Episode 700 Total Reward: 9\n",
      "Episode 750 Total Reward: 11\n",
      "Episode 800 Total Reward: 7\n",
      "Episode 850 Total Reward: 1\n",
      "Episode 900 Total Reward: 8\n",
      "Episode 950 Total Reward: 9\n",
      "Episode 1000 Total Reward: 9\n"
     ]
    }
   ],
   "source": [
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n\n",
    "Q = np.zeros([n_states, n_actions]) #You could also make this dynamic if you don't know all games states upfront\n",
    "gamma = 0.9 # discount factor\n",
    "alpha = 0.9 # learning rate\n",
    "for episode in range(1,1001):\n",
    "    done = False\n",
    "    rew_tot = 0\n",
    "    obs = env.reset()\n",
    "    while done != True:\n",
    "            action = np.argmax(Q[obs]) #choosing the action with the highest Q value \n",
    "            obs2, rew, done, info = env.step(action) #take the action\n",
    "            Q[obs,action] += alpha * (rew + gamma * np.max(Q[obs2]) - Q[obs,action]) #Update Q-matrix using Bellman equation\n",
    "            #Q[obs,action] = rew + gamma * np.max(Q[obs2]) # same equation but with learning rate = 1 returns the basic Bellman equation\n",
    "            rew_tot = rew_tot + rew\n",
    "            obs = obs2   \n",
    "    if episode % 50 == 0:\n",
    "        print('Episode {} Total Reward: {}'.format(episode,rew_tot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the Taxi game there is a reward of -1 for each action. If the agent finds itself in a state where the algorithm explored \"South\" which let to no value the Q-matrix is updated to -1. The next iteration will try an action that is not yet tried and still on 0. This approach encourages a systematic exploration of states and actions.\n",
    "\n",
    "If the learning rate alpha is set to 1 the Taxi game will be solved, the reason being that there is only one reward (dropoff passenger), so the algorithm will find it whatever learning rate. In the case of a game that has more reward places the learning rate will determine if it the agent should prioritize longer term or short term rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: |\u001b[43m \u001b[0m: :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : |\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[42mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "|\u001b[42m_\u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Reward: 9\n"
     ]
    }
   ],
   "source": [
    "# Let's see how the algorithm solves the taxi game by following the policy to take actions delivering max value\n",
    "reward = 0\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "while done != True: \n",
    "    action = np.argmax(Q[obs])\n",
    "    obs, rew, done, info = env.step(action) #take step using selected action\n",
    "    reward = reward + rew\n",
    "    env.render()\n",
    "#Print the reward of these actions\n",
    "print(\"Reward: %r\" % reward)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen lakes\n",
    "\n",
    "\"Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\"\n",
    "\n",
    "- S: Start point\n",
    "- F: Frozen zone\n",
    "- H: Hole\n",
    "- G: Goal where the frisbee has fallen\n",
    "\n",
    "In this case we have a non-deterministic environment in which the description defines the frozen lake as \"slippery\" meaning that the decisions the agent takes won't follow one single calculated path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "reward = 0\n",
    "obs = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5000 average reward: 0.0\n",
      "Episode 10000 average reward: 0.0\n",
      "Episode 15000 average reward: 0.03\n",
      "Episode 20000 average reward: 0.1\n",
      "Episode 25000 average reward: 0.16\n",
      "Episode 30000 average reward: 0.68\n",
      "Episode 35000 average reward: 0.56\n",
      "Episode 40000 average reward: 0.75\n",
      "Episode 45000 average reward: 0.74\n",
      "Episode 50000 average reward: 0.71\n",
      "Episode 55000 average reward: 0.77\n",
      "Episode 60000 average reward: 0.74\n",
      "Episode 65000 average reward: 0.74\n",
      "Episode 70000 average reward: 0.85\n",
      "Frozen lake solved\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n\n",
    "Q = np.zeros([n_states, n_actions]) #You could also make this dynamic if you don't know all games states upfront\n",
    "\n",
    "gamma = 0.95 # discount factor\n",
    "alpha = 0.01 # learning rate\n",
    "epsilon = 0.1 #\n",
    "\n",
    "for episode in range(1,500001):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while done != True:\n",
    "        if np.random.rand(1) < epsilon:\n",
    "            # exploration with a new option with probability epsilon, the epsilon greedy approach\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # exploitation\n",
    "            action = np.argmax(Q[obs])\n",
    "        obs2, rew, done, info = env.step(action) #take the action\n",
    "        Q[obs,action] += alpha * (rew + gamma * np.max(Q[obs2]) - Q[obs,action]) #Update Q-matrix using Bellman equation\n",
    "        obs = obs2   \n",
    "        \n",
    "    if episode % 5000 == 0:\n",
    "        #report every 5000 steps, test 100 games to get avarage point score for statistics and verify if it is solved\n",
    "        rew_average = 0.\n",
    "        for i in range(100):\n",
    "            obs= env.reset()\n",
    "            done=False\n",
    "            while done != True: \n",
    "                action = np.argmax(Q[obs])\n",
    "                obs, rew, done, info = env.step(action) #take step using selected action\n",
    "                rew_average += rew\n",
    "        rew_average=rew_average/100\n",
    "        print('Episode {} average reward: {}'.format(episode,rew_average))\n",
    "        \n",
    "        if rew_average > 0.8:\n",
    "            # FrozenLake-v0 defines \"solving\" as getting average reward of 0.78 over 100 consecutive trials.\n",
    "            # Test it on 0.8 so it is not a one-off lucky shot solving it\n",
    "            print(\"Frozen lake solved\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Let's see how the algorithm solves the frozen-lakes game\n",
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "\n",
    "reward = 0\n",
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "while done != True: \n",
    "    action = np.argmax(Q[obs])\n",
    "    obs, rew, done, info = env.step(action) #take step using selected action\n",
    "    reward += rew\n",
    "    env.render()\n",
    "\n",
    "print(\"Final reward:\", reward)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analizing the moves we can see that if you want to move to the rigth, there is a significant chance you move up or down. Similarly if you want to move up, there is a significant chance you move left or right. The agent has learned that if you are on the frozen tile left column second row and you want to move down it is risky to give the down command because you could move to the right into the hole, so it gives the left command because it will keep it on the tile or move up or down, but not to the right. The agent has learned to take the actions with the least risk by accidently slipping and drowning in a hole.\n",
    "\n",
    "\n",
    "It is very important to notice that this Q-Learning algorithm used thousands of iterations to find the optimal policy in a game that has a 4x4 environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Nets Applied to Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press enter to close environment...\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "reward = 0\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "input(\"Press enter to close environment...\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](./images/inverted_pendulum.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space) # Discrete(2)\n",
    "print(env.observation_space) # Box(4,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Random decision agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Global variables\n",
    "num_episodes = 10\n",
    "max_timestep = 1000\n",
    "# The main program loop\n",
    "for episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    # Iterating through time steps within an episode\n",
    "    for t in range(max_timestep):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # If the pole has tipped over, end this episode\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NNLayer:\n",
    "    # class representing a neural net layer\n",
    "    def __init__(self, input_size, output_size, activation=None, lr = 0.001):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.weights = np.random.uniform(low=-0.5, high=0.5, size=(input_size, output_size))\n",
    "        self.stored_weights = np.copy(self.weights)\n",
    "        self.activation_function = activation\n",
    "        self.lr = lr\n",
    "        self.m = np.zeros((input_size, output_size))\n",
    "        self.v = np.zeros((input_size, output_size))\n",
    "        self.beta_1 = 0.9\n",
    "        self.beta_2 = 0.999\n",
    "        self.time = 1\n",
    "        self.adam_epsilon = 0.00000001\n",
    "\n",
    "    # Compute the forward pass for this layer\n",
    "    def forward(self, inputs, remember_for_backprop=True):\n",
    "        # inputs has shape batch_size x layer_input_size \n",
    "        input_with_bias = np.append(inputs,1)\n",
    "        unactivated = None\n",
    "        if remember_for_backprop:\n",
    "            unactivated = np.dot(input_with_bias, self.weights)\n",
    "        else: \n",
    "            unactivated = np.dot(input_with_bias, self.stored_weights)\n",
    "        # store variables for backward pass\n",
    "        output = unactivated\n",
    "        if self.activation_function != None:\n",
    "            # assuming here the activation function is relu, this can be made more robust\n",
    "            output = self.activation_function(output)\n",
    "        if remember_for_backprop:\n",
    "            self.backward_store_in = input_with_bias\n",
    "            self.backward_store_out = np.copy(unactivated)\n",
    "        return output    \n",
    "        \n",
    "    def update_weights(self, gradient):        \n",
    "        m_temp = np.copy(self.m)\n",
    "        v_temp = np.copy(self.v) \n",
    "        \n",
    "        m_temp = self.beta_1*m_temp + (1-self.beta_1)*gradient\n",
    "        v_temp = self.beta_2*v_temp + (1-self.beta_2)*(gradient*gradient)\n",
    "        m_vec_hat = m_temp/(1-np.power(self.beta_1, self.time+0.1))\n",
    "        v_vec_hat = v_temp/(1-np.power(self.beta_2, self.time+0.1))\n",
    "        self.weights = self.weights - np.divide(self.lr*m_vec_hat, np.sqrt(v_vec_hat)+self.adam_epsilon)\n",
    "        \n",
    "        self.m = np.copy(m_temp)\n",
    "        self.v = np.copy(v_temp)\n",
    "        \n",
    "    def update_stored_weights(self):\n",
    "        self.stored_weights = np.copy(self.weights)\n",
    "        \n",
    "    def update_time(self):\n",
    "        self.time = self.time+1\n",
    "        \n",
    "    def backward(self, gradient_from_above):\n",
    "        adjusted_mul = gradient_from_above\n",
    "        # this is pointwise\n",
    "        if self.activation_function != None:\n",
    "            adjusted_mul = np.multiply(relu_derivative(self.backward_store_out),gradient_from_above)\n",
    "        D_i = np.dot(np.transpose(np.reshape(self.backward_store_in, (1, len(self.backward_store_in)))), np.reshape(adjusted_mul, (1,len(adjusted_mul))))\n",
    "        delta_i = np.dot(adjusted_mul, np.transpose(self.weights))[:-1]\n",
    "        self.update_weights(D_i)\n",
    "        return delta_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "####  Custom Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RLAgent:\n",
    "    # class representing a reinforcement learning agent\n",
    "    env = None\n",
    "    def __init__(self, env, num_hidden_layers=2, hidden_size=24, gamma=0.95, epsilon_decay=0.997, epsilon_min=0.01):\n",
    "        self.env = env\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = env.observation_space.shape[0]\n",
    "        self.output_size = env.action_space.n\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.997\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.memory = deque([],1000000)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.layers = [NNLayer(self.input_size + 1, self.hidden_size, activation=relu)]\n",
    "        for i in range(self.num_hidden_layers-1):\n",
    "            self.layers.append(NNLayer(self.hidden_size+1, self.hidden_size, activation=relu))\n",
    "        self.layers.append(NNLayer(self.hidden_size+1, self.output_size))\n",
    "        \n",
    "    def select_action(self, observation):\n",
    "        values = self.forward(np.asmatrix(observation))\n",
    "        if (np.random.random() > self.epsilon):\n",
    "            return np.argmax(values)\n",
    "        else:\n",
    "            return np.random.randint(self.env.action_space.n)\n",
    "            \n",
    "    def forward(self, observation, remember_for_backprop=True):\n",
    "        vals = np.copy(observation)\n",
    "        index = 0\n",
    "        for layer in self.layers:\n",
    "            vals = layer.forward(vals, remember_for_backprop)\n",
    "            index = index + 1\n",
    "        return vals\n",
    "        \n",
    "    def remember(self, done, action, observation, prev_obs):\n",
    "        self.memory.append([done, action, observation, prev_obs])\n",
    "        \n",
    "    def experience_replay(self, update_size=20):\n",
    "        if (len(self.memory) < update_size):\n",
    "            return\n",
    "        else: \n",
    "            batch_indices = np.random.choice(len(self.memory), update_size)\n",
    "            for index in batch_indices:\n",
    "                done, action_selected, new_obs, prev_obs = self.memory[index]\n",
    "                action_values = self.forward(prev_obs, remember_for_backprop=True)\n",
    "                next_action_values = self.forward(new_obs, remember_for_backprop=False)\n",
    "                experimental_values = np.copy(action_values)\n",
    "                if done:\n",
    "                    experimental_values[action_selected] = -1\n",
    "                else:\n",
    "                    experimental_values[action_selected] = 1 + self.gamma*np.max(next_action_values)\n",
    "                self.backward(action_values, experimental_values)\n",
    "        self.epsilon = self.epsilon if self.epsilon < self.epsilon_min else self.epsilon*self.epsilon_decay\n",
    "        for layer in self.layers:\n",
    "            layer.update_time()\n",
    "            layer.update_stored_weights()\n",
    "        \n",
    "    def backward(self, calculated_values, experimental_values): \n",
    "        # values are batched = batch_size x output_size\n",
    "        delta = (calculated_values - experimental_values)\n",
    "        # print('delta = {}'.format(delta))\n",
    "        for layer in reversed(self.layers):\n",
    "            delta = layer.backward(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "def relu(mat):\n",
    "    return np.multiply(mat,(mat>0))\n",
    "    \n",
    "def relu_derivative(mat):\n",
    "    return (mat>0)*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InvertedPendulum training and execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "NUM_EPISODES = 10000\n",
    "MAX_TIMESTEPS = 1000\n",
    "AVERAGE_REWARD_TO_SOLVE = 195\n",
    "NUM_EPS_TO_SOLVE = 100\n",
    "NUM_RUNS = 20\n",
    "GAMMA = 0.95\n",
    "EPSILON_DECAY = 0.997\n",
    "update_size = 10\n",
    "hidden_layer_size = 24\n",
    "num_hidden_layers = 2\n",
    "model = RLAgent(env,num_hidden_layers,hidden_layer_size,GAMMA,EPSILON_DECAY)\n",
    "scores_last_timesteps = deque([], NUM_EPS_TO_SOLVE)\n",
    "\n",
    "\n",
    "# The main program loop\n",
    "for i_episode in range(NUM_EPISODES):\n",
    "    observation = env.reset()\n",
    "    if i_episode >= NUM_EPS_TO_SOLVE:\n",
    "        if (sum(scores_last_timesteps)/NUM_EPS_TO_SOLVE > AVERAGE_REWARD_TO_SOLVE):\n",
    "            print(\"solved after {} episodes\".format(i_episode))\n",
    "            break\n",
    "    # Iterating through time steps within an episode\n",
    "    for t in range(MAX_TIMESTEPS):\n",
    "        env.render()\n",
    "        action = model.select_action(observation)\n",
    "        prev_obs = observation\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        # Keep a store of the agent's experiences\n",
    "        model.remember(done, action, observation, prev_obs)\n",
    "        model.experience_replay(update_size)\n",
    "        # epsilon decay\n",
    "        if done:\n",
    "            # If the pole has tipped over, end this episode\n",
    "            scores_last_timesteps.append(t+1)\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATARI Games and Mario Bros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set-up the virtual display environment\n",
    "# !apt-get update\n",
    "# !apt-get install python-opengl -y\n",
    "# !apt install xvfb -y\n",
    "# !pip install pyvirtualdisplay\n",
    "# !pip install piglet\n",
    "# !apt-get install ffmpeg -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "done = True\n",
    "for step in range(5000):\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](./images/mario_bros.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(0, 255, (210, 160, 3), uint8)\n",
      "Action space: Discrete(4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"BreakoutNoFrameskip-v4\")\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.grid(False)\n",
    "\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(0, 255, (210, 160, 3), uint8)\n",
      "Action space: Discrete(9)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM4AAAD8CAYAAAA/rZtiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAxM0lEQVR4nO2deXwcxZXHf6/n0q2RbEk2tvGNsWx8AQZsMPflsBjYLAvZ5QoJN4HsZsMRIAQSkiWBLBAWAsTEZEPACRgMmMNctjE+Ad+n5NvWad3XXP32jxlNd7VGM9M9M9KMqO/nMx+p+qqqrn7dXa9f/YqYGRKJxBxKfxdAIslEpOFIJBaQhiORWEAajkRiAWk4EokFpOFIJBZImeEQ0UVEtJOIKojo3lTlI5H0B5SK7zhEZAOwC8D5AA4BWAfgambelvTMJJJ+IFVPnJkAKph5DzN7AbwGYF6K8pJI+hx7io47DMBBXfoQgFN625iIZPiCJB2pZ+aSSCtSZTgUYZlgHER0E4CbAGBYfj5W3XBDiooikVjj2Kef3t/bulQZziEAI3Tp4QCO6Ddg5hcAvAAAU8vKWCHN1qa8U4YWXyTb6z9+PLENd01sC6c/q3bhhi+L+rFEmcMrsxswp8wbTj+5LQ9P78jrxxL1pNCpYuMltXFvnyrDWQdgPBGNBnAYwFUAvhfvzn4G/JxehqMa0oz0K2O6YnwPV9Owff2qufKkxHCY2U9EdwD4EIANwHxm3pqKvCSS/iBVTxww8xIAS1J1fImkP5GRAxKJBVL2xEkmz85sxPEF/nD66Z15ePtgdjh97pAu3D+5NZw+0G7DDauKw2mnwnj/nHrhmOd9PBisc/4tPOMoBrm0nsw9XxdifYMz7jIOdgXw+hkN4bQK4PyPRU/m++fUwam7VV33ZREOdWhN8MAJLTi7zBNOv3kwG8/u1DrR5YU+PHNyUzjd6idc9vngqOV6+6x65Nm1Xsbta93Y0eIIp++c0IrLRnSF059Uu/DYloJw+thcP14+rTGc9qrAxZ+K9Vp6Xp1wB75yRTGOemxRy6Vn5iAvfj29OZyu8yi4asWgcFoBY+l5Yvtd9Olg+HT9kj/PasCInEA4/cvN+fisJiucvmJEJ26foDl3tjXbcec6686djDCcY3MDmFCoGY7bIXbV8x0srFcMn4UIENZ3L9NvNSbfjyHZ2nFz7OY+LdkVMY+A0ZsA4LgCP1y668lpeN4PzRbrWVoXENZn2cR6Nnljd2jH5ftR6NTqkmUT61WWrQrH3NYsXhJOQ726xCIBCNbLpiuK3WS/P9culqGgw3BiKHL76Rmd58fYfK1w+Q6xnm6nmEebPzHnREYYzgMbCpCvu5Ar28Rir6x14qrl2hOmIyCeFJ8KYT3Q00t225oiOBUtjy3NDpihwaMIeUQyu2tWFgt35qpO8a789I48/G1vTjh9xLB+d4tdyMMfh23fuKpIuJArWsVzN78iFx8c1u7MtR7xoj3cIdYrwv0A31tRLFzIjV5zPYANjWL7eQweLpV7tp/PUJD/WO9Gtu6msKNFrOcHR7KwS7es5dtgOBsbo78y1XlsqKvr/dVABeGLOlfUY6w9Gv9rWSS8auw8voyxfmeLAzujrG/1KzHzMLK6Pvr2e9rs2NPW+2XQGYid50qTZTLS6I2VR+xz+1WM1+ojnbYeN6JEkM4BicQC0nAkEgtIw5FILJARfZxYjMv34bwhntgb6vjj7lzBHd0X/HBcm+B9ykT8DLxU0bdxZgTGzePbTe2ztDoLlTpHyAluH2aXaNdITZcNi3SfNMwyIAxnstuPB6a0xt5Qxwu7cyN6vlLJvZNbBXd0JtIV6AfDIZhu3yOdNsFwTh7kFY7x1VFHQoYjX9UkEgtIw5FILCANRyKxwIDo4ySDCQU+4Qv7/nYb2vzx31fsxJhQIIaFbDUZfZCpTCr0CemdLXZT423y7CpG5mrhMn4OfgxOZ6ThhPjr6Q1CrNr3VhRjeW38X8QHZ6n4UBeIGFCBkYuGJrWM6cqSc+sFb+GJ75Wipit+L8jJg7z4y+laIGlVh4KT3y9LZhGTjjScELVdNuiVsjyRgrKioHKwwbsJ9LGruz+p7lSEd/6ASXelRyXh3Jkxunhp94t51HsS66VIwwkx99Po4fmxqO2ypf1dMlWckmC9v6xzpfzcvb4/B6/vz4m9YZxYNjsiGkFEnxHRdiLaSkR3hZY/TESHiWhD6Dc3aaWVSNKERJ44fgD/ycxfE1E+gK+IaGlo3e+Z+XeJF08iSU8sGw4zVwGoCv3fSkTbERQilEgGPEn5jkNEowBMB7AmtOgOItpERPOJqE/Ex5jN/cxnkOD+wUOYLmfa/ZJQb0vnLtXta5KERdeJKA/AMgC/YuY3iagMQD2C5+tRAEOZ+fsR9tMreZ645vvaJuWLy9DiM2vTZuth1usV6fjJOEYmkmi9rXgcU9u+hQ4VWy+tEZYNf+qpr5j5pEjbJ+RVIyIHgDcA/JWZ3wQAZq7RrX8RwLuR9jUqeSZSjlBuiR8i5cf/9rioRQbeuUvEq0YA/gRgOzM/qVuu/+p3OYAt1osnkaQniTxxZgO4BsBmItoQWnY/gKuJaBqCz9Z9AG5OIA+JJC1JxKv2BSI/P6V6p2TAkxGRA/83+ygmF2kBlL/enC98BZ57TCcem9ESTu9tteHyZVokgFNhrJ0rKtHPeLcUqs7ul55Xh5IsLc7m1tVurNIpxNx6XBtuPk4bhbiy1onb12oOw9KsAD7SxaqpDMx4T/wavvbiGjh10STzPhuE/e1aE/z39CZcOEwbpfjq3hw8vjU/nJ7i9uIVXUxXi5cw56NSRGPFhbWCxti/f1GMLU1aAOW9k1tw1ajOcHrJ4Szc/01hOD06z49FZx0Npz2BnpECX3+nBoruFnr+0sGo0wkSPndKI04r0WYreG5nLv64WxsMN7vEg2dPaQqnazoVXPiJJnqogPG1YSaBmUtK4dXJSL19Vj1G5mmBovd8XYgPj2iyV98b1YGf6kQrNzU6cO1KUXLKDBlhOIVOxmCdyqZRVM9pg7C+0SM+CAni+kgUOVVhG4eh95djF8tgFLxTSMwjkiDhIJcqjAA1DqPOc4h55NrFg9gVMQ87xe4wFztVQZDQbhBrzDPWy5CnzVCvSIKEg1yqUBfFUKwCh3hus43tp4hl8BnzoNjt5za0n0sR88iyiXkUOkwGIxqLlIo5QM0ytayMl1x9dThtdEcPz/EjS3fB1XUpaNatz7OrQmSzVwUO6O7kBBZUHgGgotUG/ZvmqFw/7DpjOdKhoCOgLSh2BlDs0s5Vu58EQUEbMUbr7ngMCEN3AWBsnh/6a31/u02QcS3LCggG2eQl1Ovu3FkKY7gu/D7AwN4ommgAMCbPL1zIB9ttguBfiSsgGFarj4QgS4fCQsg/c09ByHH54nCKvW02BHTDCo7JDgjKqEc9iiBamGNTcUyO1n5+FdjXrs+DMc7QfpWtNkEz4thcv6CMWt2pCMNCCh2q8EbRFYAgP9yn7ui+Ql/BSLT5FVS09u4gZFAPBUsjYkP1pMFrQ4O39/UBjp2H8YIzUtNlQ01X7+u71Nh5GIkmNgiExByj6Jz44sgz1vpYQoAdgejthzja70CM9mv2iTfbRJEjQCUSC0jDkUgsIA1HIrFAWvZxHjyhRXA1GnnnUFZMMXE9dmL8YmqLsOyBDQWmBAnPHdKFc0yIHqoAHtxQGHM7Pf86sgNTinyxN0whGxsdWGhywNcvpzWbCoj5uMolzF0TCwLjl9PE9ntoY4HggIjFaSUeXDKs9w6kUzHnJEtLw7l6dGfU9bta7KYMx0bAdWM7hGUPbigwFTY4rdjX4xjRCKjmDWdOmQfzRkTxDvQBiw5kmTaca8Z0mFIoretSzBlOhPb7xaYCU0O0Jxb4TbVfLOSrmkRiAWk4EokFpOFIJBZIyz6OWYZkBXCCrlPd7qeYs58ZmVPqgUsXCvL1UQeOepMrU3TukC7hK/7KWqcQnZCO5NpVzNLFmQUY+LQ6/v5JPAxyBTCjWGu/rgBhhQlNOwCYVeJBri46YVOjIyUyU90MCMM5tcSLP8xsCqd3t9hw9tLowY9GnjypKYIgYXJP/AunNgqxanM+LMGetvQ2nKHZKl6epQWWdgWAcW8lV2hxitsn5GFFkPDX05uFsKrb1rix+JD12QhiMSAMp8lL2NioRfwebDd/wW9vdqCmSzOcRGcljsTmJocQPOqJEDCZbngCEM6tN7HYyIi0+hQhj/ou8zeTnS0OITatyeQEvmYZEIbzeU0WPjfh3ozENQmEmMfLZZ8nJnrYHxzssOM7CYo1xmJ9gzPhPG5a3SeaMGHS+z1BIklTEhXr2AegFUAAgJ+ZTyKiYgCvAxiF4NDpK5m5sbdjSCSZSDKeOGcz8zTduIV7AXzCzOMBfBJKSyQDilT0ceYBOCv0/wIAnwO4J9oOKkceWdgbZtXwk4FfNVdG1UQclZYHmcojFfiixAj2hidAUCj+RjETY5YsAiavsVgkNAKUiPYCaERwwOMfmfkFImpiZrdum0ZmjtpzI+dwRtmdlssRC5fCqLy8Wlh27BtDBM2BzIZh9CMGr5GBUT+FGAeuENtvzKIhUQOBk8Khe1M2AnQ2Mx8holIAS4loR7w76pU8YXMnWIxvMwwHMVZMWi4sPX3LHAQHNA8M40k3EurjMPOR0N9aAIsAzARQ0y1KGPpb28u+LzDzScx8EpTcRIrxrSaLVKyYtFzQTGYGVkxaDhel4KOLBEBiSp65oek9QES5AC5AULVzMYDrQptdB+DtRAspiUy+4sNn5SvgY8JpW8/EaVvPBDMwK/T/BxNXIk/xxz6QxDSJvKqVAVgUVMKFHcCrzPwBEa0DsJCIbgRwAMC/JF5MSW8QAeDgC9kXk5aFVXSWlS+H00SHXWKORJQ89wCYGmH5UQDnJlIoI2+eWY+puiDOhzcW4C97tde7S4d34vcnNYXTla12XKATtIuHVRfVojRLc7tc/2WxEGh49/GtuPP4tnB6WY0L31+lRRsMyQpg5UXaW6nKwPi3xZiu7ZdWCyMNz/u4RJB3eurkJlwyTBvE9+fKXDy6uSCcnl7kxT/O1MQBWQ2gfk/ofwBnbZuD5eXBvs5528/A0olfYMMlNSBFe7G4YtkgbGx0htMPT2nGNWO0AV6LD2Xjx+vd4fTYPD8+Oq8unPaohPLFQ4R67Z5XJQSvnvZBKWp1AZZ/ntWAM0q10bP/sz0Pz+zUhBbPLPNg/mkN4XR1pw2zPzQXa/jxeXUYnac9Xe9a58a7h7VYtevGtOOhKdoo0m8anPju8kGm8tCTESE3DgVCcKRR8E4hcb3D5DDY7n30xzB2qW2GMtiNL7mGMkQSJHTGyMNObMhDrAcZ8ujOwg7GJxO/CC//OPS/i1Q4FYYSLU9jvSh6nhxh3KzTJoorGvMwnlvjaFEF4nqzw5gj5WG8RmxJuEb0pIUgYSx3dL5DhV13Ijr8JIjqORUWQsoDDEHQMB53dKFDFU52q4/g131vyLKxoEDpUyEEFRIYbqd4LhsNgYZFTtGamr0klCHXrgqiep4AhGEHNmIU6AQL8xUfFo5bhWh8t2IW2lXt/tjiI+E7So5NVBf1qkC7rl4KMQod5urV5CVBzyHPrgrBrZ0BQldAW+8gRp4uD5UhaKDF4442tl+7n4T1LoUFUUQ/B4NLo5JCd3SfEKuCXpXg9Sbmdo0lVtdlaGwjDEJjjDIYLzgj7X4F7VHWB1jMw6/E9u00eRW0qb1v1xFQ0BHlw6DKiddLf4OJhC+OPGIRq/08KsGTYB56ZJCnRGIBaTgZjkdVcEPlDKgMXFtxIpiB6ypORICBH1ROR4eaulGQ32Yy4lVNEpkO1YY7903Fjs583LZ3GnZ15eHWvdOwsysPt+2dhq2d5rTjJPGTFoYzKs+PX8xqiL1hiFf25OCTBMe9/3l2Y0pnJ1YB3PClucFxtx3XhlMGR1F275XGGH/jZ1WdE8/r5q6Jh5dnNaT01SWO2Uxicv7QLvz7aHO6atct7H1dWhhOgYNx7tD4VTI/rTYn5BAJM6qcVojkjo7FJLfP1HlIBS0+81fpOUM8pgQJ+4MROYGknlvZx5FILCANRyKxgDQcicQCadHHSZQxeX6cWaa9vzZ5FSw6mDpNLatcO6Zd6Au8cSBbiHAwsrXJjjX1zl7XJ4PTSryYWNh7BHWhQ8UVx2rxcwEGXtmTfsNArji2Q4hw+LzGFXOax0QYEIYzpciHR3XTQOxusaWl4fx8SosQ3rKsxhXVcFbXO/HzjeZmPDDLL6c1RzWckixVOLddgfQ0nLuOb+shSCgNJwZVnQo+OOzSpdPzo99HVVlw6IIo21Mgephs2vwknFtfP+gFxMOKWhd2t2iGk+prYEAYzpp6F9aYmC+nv7h1Td+K5iWD6k4bfrA69WKNifKAybmIEkU6ByQSC0jDkUgsYPlVjYgmIKjY2c0YAA8BcAP4IYDuYYP3M/MSq/lIJOlIIkOndwKYBgBEZANwGEGlmxsA/J6ZfxfvsXxqcF7I3nA7xYFQpsuK6MdPBf0hmthf1HcpPUZcpjs+NbEZDZLlHDgXQCUz7ycLEXnbmx2Y/l7v86G8c3Y9phdbn43Zq1LU40sS48QlmXduNzU6MC+B2SOSdRu+CsDfdOk7iGgTEc0nosxzJUkkMUjYcIjICeBSAH8PLXoOwFgEX+OqADzRy343EdF6IloPNdqAYYkk/UjGE+diAF8zcw0AMHMNMweYWQXwIoLqnj2QSp6STCYZfZyroXtNI6KhzFwVSl6OoLpnionUEzf2tYzbxFpv3CbR9fFuk24ko16pOHf9e24TnVgqB8D5AG7WLX6ciKYhWJN9hnUp4bIRXXjm5KZwuqLVjrOXaoKELoVRcZkoLzTyTVEeav3cWpRlaaPP/u2LYizXCRL+58Q23D1REyT8rMaFa3XTHw7JVrHuYk2QMMDAqEWiIGHlZdWC/NOZH5VgTwrjqZLBuPwAPjtfL0jYc/Lc/VdUC68uJy0pFWZ8/uvpDZhTqo1sfWJbHv5nhyZIeHaZB6/M1k2e26lgpm7yXIUY+w3yXmPfEuWhll1QhzF5WsjN7WvTePJcZu4AMMiw7JqESmQR0ZnX8+4Tj7Mv6jYU+xj69b2pzyZjGHBfk/C5M7k+0qbJKEMySe/bXZy8fTAL7x3SZFmN16xHBUa/Kcq2Gkc2n/p+qdBgfsNBfr8tD09v18biG/ev7lR65GHk+LfF9b4kfeuxE/DJrInCsnO+3J6Ub0kVrbaY9Rq3KHq9rl1ZLDyRjOX6vMYl5GEstso928+Yx7lLS6K2X7IZEIbDoBgXYaz1EFQ7I6GCoCaYRyoii10KYcmpx8NhECf86LSJmLt6BzzRCx0HidcrwIRok6H1RfslGxmrlsHk2RS8PXMCHES4aNUOXLRqB5gZF6/agcvX7sLCk8Yj1yabOBUMiCfOtxUioMBhg09ltPoDeGvmcQCAFn8Ab5w8HoOd9rT32WUq8nY0QGAAt23aG07fsXkfvAm/pkl6Iy2eOOML/Hj23Lpe1+vnPbGCU2G8e3Z9QscwiwrgIpNz9FjFTsCC6WPD6QXTx4IBOPoo8vKDc+v6/A78nU8HJ9RnnFjox0dRrjkAuGBB7+vSwnCybYxyd+qm3CMgpcePhBVBQqsQEcbnacqm4/ISUzk1y8RCf58LEnbPRGeVHHti15x8VZNILCANRyKxgDScDMenqvjN7iNQmfHYrsPg0N8AMx7ffQRd0kGQEtKij5Mok90+QTSvvkvB/+4yp7jfF9w/uUWYO/TZHbk46rUuY9QVYDyztwbvVDciWyG8W9OEPLsN79Y0Icem4J2axoSjBwa7Arhtgjbsw68Cj20piLJH/3D7hDYMcmkdyzf2Z2NrsyNl+Q0IwxmX78dN47XG3d1iS0vDuXFcuyBI+H97cnDUyqweIXzM+MeR4PQor4f+vnY4OCv1wiPxT5sSDbeThXPbFUhPw7lyZIcgSLihwSENJxaVrXa8uDsnnK73pKcg4cuVubDpoj+bY0ypManQjx+MS+0gv/LC6EPSm7wknNu+Dm2Jl7/vz8Egl2Y4la2pvbQHhOFsbnJgc1PfCtJZ4Zebzd2pTy3x4tSSBB5JSaDeY8MvNqX/uf3Dzr59w5DOAYnEAtJwJBILSMORSCwQs49DRPMBXAKglpknh5YVI6jiOQrB4dFXMnNjaN19AG4EEADwI2b+MFYenQHC9ub4u1uNCQjJdWMmPytY+XxypMOW8nLF4rAFlf8dzfaUCxJGm4okHhq9SlLPLXGMqZeJaA6ANgCv6AzncQANzPwbIroXQBEz30NE5QgKd8wEcAyAjwEcx8zRxjGBnMMZZXcmXptecCmMSsOY9WPfEDUHMhvG8OKucOpQQxaGF3fhcEPWgJiuXSHGgSvE9huzSNQcSAmH7v2KmU+KWKZY+zLzcgDGjwLzAHTHji4AcJlu+WvM7GHmvQAq0Is8lCRZMIa6PVjy03XhX2mBF3+6aROOKe4CJRIJKekVq+88Zd0SUKG/paHlwwAc1G13KLSsB1KQMDlkOVR8eO9aAAAz0Nxhx9L71uCqZ6ZjwS0bkeuK+rCXWCTZzoFIz86ItzwpSJg8OrwKOkP9vgt+fQoYwLIHV+M7vz0ZbZ4B8aku7bB6Vmu6hQeJaCiAbkGxQwBG6LYbDuBIIgUEABuJb+oqQ+ifELjHeBCzX7jtBj2nAEPoHyhgoQPMCIpQ6JfYY5TBmEdQiUXbpkc9AagcvZ5dPhtOfeh0EDHW//ILfPHwl8IxYtaLWLh79km90rL9zGHVcBYDuA7Ab0J/39Ytf5WInkTQOTAewFrLpQvx1llHhdkKfvZNARboJnCdN6ILf5jZFE7vbrHh7KWlMMPqi2sxJFsLEvzeClGQ8MflbfixTpDw0+qegoTr5+oECVVgpEGQcOe8aiFWbc6HoiDh0yc3Yd4IrZP/p4ocYfLc6cU+LD77aDjd5CVMficom8RMOPFnZwAANvx6efiS2XhJDQqd2kV1yaeDsKFRm8n6kaktuH5sRzi96EAW7lyn6eSPyw/g8wu0kZJdgZ6ChJWXVwsX/onviYKEr8xuwJllWgTE77aKgoRnlXnwl9N1goQdCk5+39wMCJ+cX9dj8ly9IOH1YzvwiG4S4K+OJjZbQTzu6L8BOAvAYCI6BODnCBrMQiK6EcABAP8CAMy8lYgWAtgGwA/g9lgetXiJ4fwzrLd2J4maB5srQ2+bmquH2fWRV0bbhxG73LHKFM82yTh3qS6DGWK6o/uC2O7oxLSH43NHp4N2tPX1RIwNj60I/R9cc/KDs+GJOB185tQLiNcdnYx6GIjijs6QnmOsJ0gy/PmJ5hFPGZKfR5YjgNWPrAyujbh7ZtbLPH2Rh0aGGI4kGpG+2p/8wGx4/DKiKlXIM5vhdPkUzHp4lrDslIe6jSbzowbSFfnEyXgIbV02zNYZT/CbjjSaVJIWhjOx0Ie/fqcm7u1/syUfC/fnxN4widxyXJswhHhlrVNw28bD6otq4bRpndTLPx+E/e3xN8EJbi8W6OaRscI1XxSbGlI8Ks+PN8/UXOCeAOG0D8y5+p+d2YjTdAPyntuVixd39+3As6tHdeC/JrWa2mfGH3tflxaG41CA0qz4FfyybX3vCcy1s1BG/beReCnJCgjfccyK+Jk9T5GPYa7cdhLz7LLwccHtVIVj5PZD+2XbOOFzp0f2cSQSC0jDkUgsIA1HIrFAWvRxEuWUwR78YJwWb1XVqeAhXYxXPPx2RhPcun7L77fnYZuJTrTbqeK3M5rDaZWBm9eYcx78cHwbZg7SYvI+rnbh9X2pdYJcPaoD5wzxhNNr6p14qcJctPofT2kUviX95KtCNEeMWIjMZLcPdx2vxQE2egk//dptqgyPTm0WYg1f2J2LdUedUfZIjAFhOEOzVVw8TAuO3N1ifvjv2UM8won/yx5zF2yWjYUyWJmtYFqRTzjGkc7UvxBMcot5Bjv/5gznomFdgqPjgQ0FaI4u1yZQ4goIZajqMF/vOWUeIcjznUOpnbFhQBjOpkYHHtygaZY1WdAk+O3WfOTYtSdOhUlBuxYvCWWwEgK4cH821uvuktv6QH/g3UNZgnifFSG/n28sEL4atcQQWjSyq9UunLt2v/lvUE/tyEOhQzvpm5tSp+IJDBDD2dNmF8LzrfB6gt+FOgIKXq5MbEDesposLIv/c1ZSWF3vwup6V+wNo/DnBOt9uMOOlysTa783D/Ttdz3pHJBILCANRyKxgDQcicQCadnHWVbjhD+KZtahDnNeM5WBT6rE9/j+H75nnmaf0qMeZmkx4SZOFzhC+5kVfDzYYYt67uwKC8O7Y2FVyfO3AP4JgBdAJYAbmLmJiEYB2A5gZ2j31cx8S9ylCXHrmqKkNrCPCdd9WRx7wzSnstU+IOphFkbi7be0KgtLq3p3URc6VGy9NH7PTDxX558BXGQsB4DJzDwFwC4A9+nWVTLztNDPtNFIJJmAJSVPZv6ImbvFfFcjKAMlkXxrSMb70PcBvK9Ljyaib4hoGRGd0dtOeiXPo52dvW0mkaQlCTkHiOhnCMpA/TW0qArAscx8lIhOBPAWEU1i5hbjvsz8AoAXAGBqWVnUrl6+QxVE8Tr8BI/OeeBUGLm6r/4BNnaCGUWG8TONXoJ+lGShQxXirVp9ZEoUj8BCrFswD3P3pVy7CqduF08g+GG1GxsxCnRfxxmxoyTcTrXHV329EF+OTRXGCHlVoN2kVkGRU4wvavKSKbF3BzHydPVSGYZYN/Pt1+4nQQXHpbAQGeJnoDWBfrRlwyGi6xB0GpzLIY0pZvYA8IT+/4qIKgEcB2C95RICePX0hqiChHOHRRckdCnA5n8SO35BeSiNT86viypIGIuyOAQJY/H4jOaogoRTi3oXJOyNlRfWRhUkvP+E1qiChPGw4ZKaqIKEsTi9NLogoUI92y8oD6WlF59dH1WQ8N9G97EgYSSI6CIA9wA4k5k7dMtLEJz+I0BEYxBU8txjuXQhfGrw7tuN0RWpsrjeF8GV7YkxctGnkrCNaXc1Ry9jPPhZLIPRJc+GPOKZ5iJYL/EpJeRpOLdWJsf1BtBDXtYMKih6vdh8+xnPfyCOa8QMVpU87wPgArCUgmJe3W7nOQAeISI/ghNL3cLMCc8bfsWy6HeGxYeyhbuLEY9KGPtW9Lu/2XH0Rqq7bDHziMVd69y4a5271/XfNDpN5zHtvehSsg9vKsTDCU6OO/7txOq9rMYVtV4qYrffeR+XRF2/YE+u8JaSKDENh5mvjrD4T71s+waANxItlESS7mTeZ2SJJA2QhiORWEAajkRigbQM8jTLpcM78eRJTab2Oe6tvp88t3yx6Dr2mhxePa3Ii3/oxAGtcMWyQdjUGP9Y/MpWG8Yuiu7yTjUKGLsuq469oY671rnx3uHeHUaJMiAMRyEgy7zMQJ/jSdAFmox6mn3FYBA8ydPxs4aFepsVezSLfFWTSCwgDUcisYA0HInEAhnRx3nyxCYcV+APp5/flYt3k9zxe2V2A4p1wYoPbizANw3xd6IHuQJYMEuLt1IBXPqZGPGw6Mx6OHS3qptXF+Fwp/by/tNJLZhTqo1CXHwoCy+kWNX/luPacIlO0+zzGhd+t02b2HZ4jh/Pn9IUTnvVnpEc75xdL7hZrl1ZhAZv/J2SE4u9+MVULY6s3qPg+iQP2Lt0eKcw28SOFjt+8pXb8vEywnDGF/gxTRfkOciV/N5qeaFPCPLMt5uLuHIoEMoYSZBwSpFPiER2GVT7R+YGhGN81ZBabTAAGJ4j5rm3Tbzgs2xivSLNVjClyCd0xh0m32MKHKqQhxVBwlgMdol5BCzEEurJCMN5dFMBCnVPgx0pEOr7j/Vu4ULe2mQuj0aPghu+1KKKIwkS3rS6SAiGrDYodT6/Kw9vHdSepAfaU+8q/L89OVhWo0WBV3WKeVZ1ivWKdMHd+GWRMP+oWUHITU0OIY+uQPJdYkurXDjYoeVhRbRST0YYztoUagB3Y2YIQSQ8KkUd0w4An1RHX7+5yZFyBUojO1oc2NHSe57tfiVmvT6OUa9YHPXYsLQqtTeJgx12HOxI3uUunQMSiQWk4UgkFpCGI5FYICP6OOnIsJwArh3THnvDKLx1MDvtBQILHSrmjUhMTOWY7P6O2QGOL/Bh5mDN1V/XpeD9I9Y/aUjDsciEAj8em95Dg8QUX9S60t5wSrLUhOuZDswq8fbQHEjEcGK2GhHNJ6JaItqiW/YwER0mog2h31zduvuIqIKIdhLRhZZLJpGkMVaVPAHg9zrFziUAQETlAK4CMCm0z/8SUQbELUsk5rCk5BmFeQBeY2YPM+8FUAFgZgLlk0jSkkT6OHcQ0bUIaqb9JzM3AhiGoCRuN4dCy3pARDcBuAkAhuXnR9qkTxmV64dddxs50qEIYoB9QVlWAPk6Yb4mL6Hek9oHdokrIOiutfrIlCZaMsixqTgmR3Mg+FVgX3t6d7+tlu45AI8iKKH1KIAnEJTCjRQrETEqyIySZ1/wjzOPJiRImAwemtISVZAwFdw1sS1hQcJEOWWwN6ogYTpiyXCYOSyrSEQvAng3lDwEYIRu0+EAjlguXR/S6FWEJ46vHzyobT5CvUcrhFkpWkt5+sU8W/sgTyNeVSxDQ4JxZJHoCoh5mJlOPhJWlTyHMnNVKHk5gG6P22IArxLRkwCOQVDJc21CJewjzo8haNcX3PONG/d807d5/mZLAX6zpSD2hilkZZ0L095N7RPm1X05eHVf8ibYtarkeRYRTUPwNWwfgJsBgJm3EtFCANsQFGO/nZljiJdKJJlHUpU8Q9v/CsCvEimURJLupPdna4kkTUlvn58JIg0cy8Q80rEM39Z6R4M4DUo0tayMl1ytvRGWLy4zGcNlpQ5mRxmm6jyZKUeyytAfeSZSBiDVbRxp8tzhTz31FTOfFGn7AfLE6QtFzr5V/YxMf5QhHeoNpE85gsg+jkRiAWk4EokFpOFIJBbIiD7Oy7MaMLlQ08R6fFs+/r5f+wp80TFdeHRqczi9t92OK5cPCqedCmPlhdrEtgAw8/1SYWbkJefUo8Slfau9Y50ba+q1WLWbxrfhh+O0EZ8r61y4e707nC7NCuC9s+vD6QAIp74vTo+48sJaOBWtk/vPywfhgC6Y8VfTmnHBUC1W7bV9OXhiuxYAe4Lbh/mnaYHqLT4F58aIePj0/Drk27X4oeu/LMbWZk3V5r/KW3HlSC1W7f0jWXhIFx83KtePv8/RZkjwqITTPxTrtebiGuEOfPGng4Xg1GdObsSputGXf9ydh5cqtGkFZ5V48JRutomaLhsu0Yk5KmCsuVhsv1kflgrzeP5jzlGMzNVEK+/fUCio8/zryA78pLw1nN7U5MCNq6yLHmaE4Qx2qRiqi57NMQj5ZdlYWN/mF4MVCBDWdy/TH6U0KyAEeboMz+J8h5hHsUEUUSExj0iChEOyA4Igod3Q33U7xXoWGKZBdyhiGbK9sT1NZVli9LNDEfcpNORpnHrdroj1iiRIOCRbFQQJjTMFFBvaL88u5uEy1KsHFLn99JRkBcRzY7hGcu1iHkc6EwtGzAh39Jg8v3AiqjoVQWK10KFieI7Woh4VqGjV7qoERnmhdjcCgK3NduhP/4QCn3Ah72+3oU0X8FiaFUCJS2+chP26p4WdGBMKjHmIemXlhT6hwXe32oUZlofn+FGoG1Zw1KugWicQmG1TMSZPq2eAEVUTDQAmFvgEEcQ9bTZ06oZLDM0OCNK/zT7CIZ3+mFNhjM/X6sUAthnqNUn3NgAAO1vswuzVI3P9yNMpo9Z2KajTPZHy7CpG5mr18jOwU6gXY5Kh/bY124U3hvH5Pjh1N7tDHTYhkLPYGcBQ3Y2xM0DY06bV06w7OiMMRyJJNWYNR16dEokFpOFIJBaQhiORWCAtvWo/KW9NeL5MicQMLsVcXz8tDef74zpibySR9CPxjACdD+ASALXMPDm07HUAE0KbuAE0MfM0IhoFYDuAnaF1q5n5llh51Hd0YP6GDaYLL5H0FzHd0UQ0B0AbgFe6Dcew/gkAzcz8SMhw3o20XYw8TPvEx40LRg50damYODH4Fbqx0Yf16+OXaz3vvEH47LOjOOOMYjgcwVfDVaua0NYmR3v3FzmKglluN3zMWNbYGHuH1GJ9WAEzLw8ZRA+IiABcCeCchIpnkjFjsjF7thsAwe2245JLgiEge/d2wOs9gE2bWqPuDwCzZrkxa5YbHo+KH/94FHJygh/knnvuAJYurUdrqzSeviZHUXCa241Zbje8qgqvqoIBrG5ujrlvX5OoV+0MADXMvFu3bDQRfUNEy4jojASPH5ETTsiH388oLnbglFPc2Lcv2CcaPToHN988IsbeQS69tBTvvVeHe+4Zg4qKdni9wa/Kt956LEaMSO7EvJL4KLTbMS0/H4/s2YP/PXgQD4wZg38qKcG0NBCsNJKo4VwN4G+6dBWAY5l5OoD/QFAqKqL2EBHdRETriWi92UzffrsWLpeCuXNLsHNnO5Yta0Rrqx/bt7dh7974pqS4995dePTR8XC5FLz6ahU6OwOoqOjA9u1t6OyUT5v+oMrrxRP794fTXlXFQ5WVuHboUEzISZ60UzKw7FUjIjuAKwCc2L2MmT0APKH/vyKiSgDHISiTK6BX8rTSx+lm6tR8DB+ehU8/PYo//OGApWPcdtuxaGz046GHdqOuzht7B0lKsQE4JisLB7q6EGDG0wcO4LmJE3HxN30sOheFRNzR5wHYwcyHuhcQUQmABmYOENEYBAUJ9yRYxqh8/nkDnn56f+wNo3D33dtlnyZNsAEYm5OD20eMwI927MBwlwvPl5eDmVHmdKLGmx43tnjmx/kbgFUAJhDRISK6MbTqKoivaQAwB8AmItoI4B8AbmHmeGc6sITLpaCoyI7cXOtC4W63A0VFdigyjqLfKXO58POxY/FARQVcioKXJk0CABARFkw25axNKVYFCcHM10dY9gaANxIvVvxccMFgXHDBYKxd24THHtsDVWV0mhxrMX/+CQCAW27ZiupqDzo7A1D7f/a9bx0Kgp61QQ4HXiwvx63bt6PN70ee3Q5mRnsgfd4KMvYe6/cz/H7t6p4504233pqB3/3u+LiP4fGo0H/Hev75SXjrrRk4/vi8pJZVEh+jsrPxh4kTAQAFdjteKi/HVZs3o0tV4VFVfHfTpn4uoUbGGs4rrxzByy8fRiCgXfjMLKRjcfnl36C9PSAYTyDASIcxSt9GGEAgdO6ZGf7Q77ING3DZxo39WzgDGWs4ALBwYTWeeWY/VJWhqowNG1rxox9tN3WMyy//Bi0t/vAxbr11K7ZvT2w2aYk19nZ24ofbtkFlRrPfj39JoyeMkbQYAZqIO1oiSSFyBKhEkkyk4UgkFpCGI5FYIC0Hskn6l1MnFuGR64Nu4QO1nfjBE+kT6pIuyCeOROCsqYPxi+uOx+7Dbbj3pa0YNywXL//X9P4uVtohDUcS5sKTSvHTfx2HLftacd9L27CpsgV3PbsJo4dK4zEiX9UkYbKcCvJzHPD6VbR0BJUzm9p8sCmE4gJnP5cuvZBPHInEAtJwJGFWbD6K+R/sx5QxBbj7n8fi2NJsPHTN8Tja4sU9L2zt7+KlFfJVTRKmodWHxV9WAwx8d84xmDA8D4W5djy8YAd2HGzr7+KlFTLkRtKDQQVOnDA6OOK9w+PH2h1N/Vug/iO9ZyuQhiNJU2SsmkSSTOIZOj2CiD4jou1EtJWI7gotLyaipUS0O/S3SLfPfURUQUQ7iejCVFZAIukXmDnqD8BQADNC/+cD2AWgHMDjAO4NLb8XwH+H/i8HsBGAC8BoAJUAbDHyYPmTvzT8re/tmo35xGHmKmb+OvR/K4La0MMAzAOwILTZAgCXhf6fB+A1ZvYw814AFQBmxspHIskkTPVxQlK40wGsAVDGzFVA0LgAdE9FPAzAQd1uh0LLJJIBQ9zfcYgoD0EFm7uZuSUoGx150wjLOMLxbgJwU7z5SyTpRFxPHCJyIGg0f2XmN0OLa4hoaGj9UADdE9EfAqAXcB4O4IjxmMz8AjOf1Ju7TyJJZ+LxqhGAPwHYzsxP6lYtBnBd6P/rALytW34VEbmIaDSCap5rk1dkiSQNiMOrdjqCr1qbAGwI/eYCGATgEwC7Q3+Ldfv8DEFv2k4AF8eRR397T+RP/iL9evWqycgBiaR3ZOSARJJMpOFIJBaQhiORWEAajkRiAWk4EokF0mUEaD2A9tDfgcJgDJz6DKS6APHXZ2RvK9LCHQ0ARLR+IEURDKT6DKS6AMmpj3xVk0gsIA1HIrFAOhnOC/1dgCQzkOozkOoCJKE+adPHkUgyiXR64kgkGUO/Gw4RXRQS9aggonv7uzxWIKJ9RLSZiDYQ0frQsl7FTNINIppPRLVEtEW3LGPFWHqpz8NEdDjURhuIaK5unfn6xAr5T+UPgA3B4QdjADgRFPko788yWazHPgCDDcsiipmk4w/AHAAzAGyJVX5YEGNJk/o8DOAnEba1VJ/+fuLMBFDBzHuY2QvgNQTFPgYC8xBZzCTtYOblABoMi3sr/zykuRhLL/XpDUv16W/DGSjCHgzgIyL6KqSlAPQuZpIpDEQxljuIaFPoVa771dNSffrbcOIS9sgAZjPzDAAXA7idiOb0d4FSSKa22XMAxgKYBqAKwBOh5Zbq09+GE5ewR7rDzEdCf2sBLELwUd+bmEmmkJAYS7rBzDXMHGBmFcCL0F7HLNWnvw1nHYDxRDSaiJwArkJQ7CNjIKJcIsrv/h/ABQC2oHcxk0xhQImxdN8EQlyOYBsBVuuTBh6QuQjK6lYC+Fl/l8dC+ccg6JXZCGBrdx0QRcwk3X4A/obg64sPwTvwjdHKD5NiLGlSn78A2Iyg6MxiAEMTqY+MHJBILNDfr2oSSUYiDUcisYA0HInEAtJwJBILSMORSCwgDUcisYA0HInEAtJwJBIL/D9ozzDD894sUAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"MsPacmanNoFrameskip-v4\")\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.grid(False)\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(0, 255, (210, 160, 3), uint8)\n",
      "Action space: Discrete(6)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"SpaceInvadersNoFrameskip-v4\")\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.grid(False)\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "327.102px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
